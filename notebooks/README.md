# âœˆï¸ OpenSky ETL Pipeline with Azure Databricks

This project implements a scalable ETL pipeline using **Azure Databricks** to ingest real-time flight data from the **OpenSky API**, process it using the **medallion architecture (Bronze â†’ Silver â†’ Gold)**, and store the results in **Azure SQL** for downstream analytics with **Power BI**.

---

## ğŸ“Š Architecture

```
OpenSky API â†’ Azure Databricks
     â†“
 Bronze â†’ Silver â†’ Gold (Delta Lake)
     â†“
 Azure SQL / Azure Storage
     â†“
     Power BI
```

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ notebooks/               # Databricks notebooks (Python or ipynb format)
â”‚   â”œâ”€â”€ 1_bronze_ingest.py
â”‚   â”œâ”€â”€ 2_silver_transform.py
â”‚   â””â”€â”€ 3_gold_model.py
â”œâ”€â”€ requirements.txt         # Python dependencies for Databricks environment
â”œâ”€â”€ .github/workflows/       # (Optional) GitHub Actions for CI/CD
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1. Clone this Repo
```bash
git clone https://github.com/your-username/opensky-etl-pipeline.git
cd opensky-etl-pipeline
```

### 2. Upload Notebooks to Databricks
- Open your Databricks workspace.
- Import notebooks from the `notebooks/` folder.

### 3. Configure Azure Resources
- Set up **Azure SQL** or **Azure Storage** for data persistence.
- Generate and store **Databricks secrets** for:
  - OpenSky API authentication (if needed)
  - Azure Storage keys / SQL credentials

### 4. Install Python Dependencies
In a Databricks cluster:
```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Key Components

| Layer   | Description                                  |
|---------|----------------------------------------------|
| Bronze  | Raw ingest from OpenSky API (JSON format)    |
| Silver  | Cleaned and structured data                  |
| Gold    | Aggregated data ready for BI consumption     |

---

## ğŸ“ˆ Power BI Integration

- Connect **Power BI Desktop** to **Azure SQL Database** using the same schema used in the Gold layer.
- Refresh dashboards on a schedule or via pipeline triggers.

---

## ğŸ“¦ Dependencies

Main packages used:
- `pyspark`, `delta-spark`
- `requests`, `pandas`, `numpy`
- `azure-identity`, `azure-storage-blob`, `pyodbc`

See [`requirements.txt`](./requirements.txt) for details.

---


